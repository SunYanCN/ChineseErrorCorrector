#  ä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•çº é”™
[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/README.md) 

<div align="center">
  <a href="https://github.com/TW-NLP/ChineseErrorCorrector">
    <img src="images/image_fx_.jpg" alt="Logo" height="156">
  </a>
</div>



-----------------



## ä»‹ç»
æ”¯æŒä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•é”™è¯¯çº æ­£ï¼Œä»¥åŠæ‹¼å†™å’Œè¯­æ³•é”™è¯¯çš„å¢å¼ºå·¥å…·ã€‚

## æ–°é—»
[2025/01] æ–°å¢äº†åŸºäºQwen2.5çš„ä¸­æ–‡æ‹¼å†™çº é”™æ¨¡å‹ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰é”™è¯¯çº æ­£ï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector-7B](https://huggingface.co/twnlp/ChineseErrorCorrector-7B)ã€‚

[2024/06] v0.1.0ç‰ˆæœ¬ï¼šå¼€æºä¸€é”®è¯­æ³•é”™è¯¯å¢å¼ºå·¥å…·ï¼Œè¯¥å·¥å…·å¯ä»¥è¿›è¡Œ14ç§è¯­æ³•é”™è¯¯çš„å¢å¼ºï¼Œä¸åŒè¡Œä¸šå¯ä»¥æ ¹æ®è‡ªå·±çš„æ•°æ®è¿›è¡Œé”™è¯¯æ›¿æ¢ï¼Œæ¥è®­ç»ƒè‡ªå·±çš„è¯­æ³•å’Œæ‹¼å†™æ¨¡å‹ã€‚è¯¦è§[Tag-v0.1.0](https://github.com/TW-NLP/ChineseErrorCorrector/tree/0.1.0)

### è¯„ä¼°ç»“æœ
- è¯„ä¼°æŒ‡æ ‡ï¼šF1


| Model Name       | Model Link                                                                                                              | Base Model                 | Avg        | SIGHAN-2015 | EC-LAW | EC-MED   | EC-ODW |
|:-----------------|:------------------------------------------------------------------------------------------------------------------------|:---------------------------|:-----------|:------------|:-------|:-------|:--------|
| twnlp/ChineseErrorCorrector-7B        | https://huggingface.co/twnlp/ChineseErrorCorrector-7B/tree/main                                    | Qwen/Qwen2.5-7B-Instruct | 0.712     | 0.592      | 0.787 | 0.677 | 0.793     |
| twnlp/ChineseErrorCorrector-32B        | https://huggingface.co/twnlp/ChineseErrorCorrector-32B/tree/main                                    | Qwen/Qwen2.5-32B-Instruct |     |       |  | |     |

## ä½¿ç”¨

```shell
# pip install transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
checkpoint = "twnlp/ChineseErrorCorrector-7B"

device = "cuda" # for GPU usage or "cpu" for CPU usage
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)

input_content = "ä½ æ˜¯ä¸€ä¸ªæ‹¼å†™çº é”™ä¸“å®¶ï¼Œå¯¹åŸæ–‡è¿›è¡Œé”™åˆ«å­—çº æ­£ï¼Œä¸è¦æ›´æ”¹åŸæ–‡å­—æ•°ï¼ŒåŸæ–‡ä¸ºï¼š\nå°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚"

messages = [{"role": "user", "content": input_content}]
input_text=tokenizer.apply_chat_template(messages, tokenize=False)

print(input_text)

inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)
outputs = model.generate(inputs, max_new_tokens=1024, temperature=0, do_sample=False, repetition_penalty=1.08)

print(tokenizer.decode(outputs[0]))

```