#  ä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•çº é”™
[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/README.md) 

<div align="center">
  <a href="https://github.com/TW-NLP/ChineseErrorCorrector">
    <img src="images/image_fx_.jpg" alt="Logo" height="156">
  </a>
</div>



-----------------



## ä»‹ç»
æ”¯æŒä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•é”™è¯¯çº æ­£ï¼Œå¹¶å¼€æºæ‹¼å†™å’Œè¯­æ³•é”™è¯¯çš„å¢å¼ºå·¥å…·ï¼Œè£è·2024CCL å† å†› ğŸ†ï¼Œ[æŸ¥çœ‹è®ºæ–‡](https://aclanthology.org/2024.ccl-3.31/) ã€‚

## ğŸ”¥ğŸ”¥ğŸ”¥ æ–°é—»
[2025/01] ä½¿ç”¨38ä¸‡å¼€æºæ•°æ®ï¼ŒåŸºäºQwen2.5è®­ç»ƒä¸­æ–‡æ‹¼å†™çº é”™æ¨¡å‹ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰é”™è¯¯çº æ­£ï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector-7B](https://huggingface.co/twnlp/ChineseErrorCorrector-7B)ï¼Œ[twnlp/ChineseErrorCorrector-32B-LORA](https://huggingface.co/twnlp/ChineseErrorCorrector-32B-LORA/tree/main)

[2024/06] v0.1.0ç‰ˆæœ¬ï¼šå¼€æºä¸€é”®è¯­æ³•é”™è¯¯å¢å¼ºå·¥å…·ï¼Œè¯¥å·¥å…·å¯ä»¥è¿›è¡Œ14ç§è¯­æ³•é”™è¯¯çš„å¢å¼ºï¼Œä¸åŒè¡Œä¸šå¯ä»¥æ ¹æ®è‡ªå·±çš„æ•°æ®è¿›è¡Œé”™è¯¯æ›¿æ¢ï¼Œæ¥è®­ç»ƒè‡ªå·±çš„è¯­æ³•å’Œæ‹¼å†™æ¨¡å‹ã€‚è¯¦è§[Tag-v0.1.0](https://github.com/TW-NLP/ChineseErrorCorrector/tree/0.1.0)

## æ•°æ®é›†

| æ•°æ®é›†åç§°     | æ•°æ®é“¾æ¥                                                                                                      | æ•°æ®é‡å’Œç±»åˆ«è¯´æ˜                                                                                                            | æè¿°          |
|:--------------|:-----------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:------------|
| CTCï¼ˆæ‹¼å†™çº é”™æ•°æ®é›†ï¼‰ |[twnlp/csc_data](https://huggingface.co/datasets/twnlp/csc_data)  | W271Kï¼š279,816 æ¡ï¼ŒMedicalï¼š39,303 æ¡ï¼ŒLemonï¼š22,259 æ¡ï¼ŒECSpellï¼š6,688 æ¡ï¼ŒCSCDï¼š35,001 æ¡ | ç”¨äºä¸­æ–‡æ‹¼å†™çº é”™çš„å¤šç±»åˆ«å¤§è§„æ¨¡æ•°æ®é›† |



## æ‹¼å†™çº é”™è¯„æµ‹
- è¯„ä¼°æŒ‡æ ‡ï¼šF1


| Model Name       | Model Link                                                                                                              | Base Model                 | Avg        | SIGHAN-2015 | EC-LAW | EC-MED   | EC-ODW |
|:-----------------|:------------------------------------------------------------------------------------------------------------------------|:---------------------------|:-----------|:------------|:-------|:-------|:--------|
| twnlp/ChineseErrorCorrector-7B        | https://huggingface.co/twnlp/ChineseErrorCorrector-7B/tree/main                                    | Qwen/Qwen2.5-7B-Instruct | 0.712     | 0.592      | 0.787 | 0.677 | 0.793     |
| twnlp/ChineseErrorCorrector-32B-LORA        | https://huggingface.co/twnlp/ChineseErrorCorrector-32B-LORA/tree/main                                    | Qwen/Qwen2.5-32B-Instruct |  0.757   |    0.594   | 0.776 |0.794 |   0.864  |

## ä½¿ç”¨
### transformers 
```shell
# pip install transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
checkpoint = "twnlp/ChineseErrorCorrector-7B"

device = "cuda" # for GPU usage or "cpu" for CPU usage
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)

input_content = "ä½ æ˜¯ä¸€ä¸ªæ‹¼å†™çº é”™ä¸“å®¶ï¼Œå¯¹åŸæ–‡è¿›è¡Œé”™åˆ«å­—çº æ­£ï¼Œä¸è¦æ›´æ”¹åŸæ–‡å­—æ•°ï¼ŒåŸæ–‡ä¸ºï¼š\nå°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚"

messages = [{"role": "user", "content": input_content}]
input_text=tokenizer.apply_chat_template(messages, tokenize=False)

print(input_text)

inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)
outputs = model.generate(inputs, max_new_tokens=1024, temperature=0, do_sample=False, repetition_penalty=1.08)

print(tokenizer.decode(outputs[0]))

```

### VLLM

```shell
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("twnlp/ChineseErrorCorrector-7B")

# Pass the default decoding hyperparameters of twnlp/ChineseErrorCorrector-7B
# max_tokens is for the maximum length for generation.
sampling_params = SamplingParams(repetition_penalty=1.05, max_tokens=512)

# Input the model name or path. Can be GPTQ or AWQ models.
llm = LLM(model="twnlp/ChineseErrorCorrector-7B")

# Prepare your prompts
prompt = "å°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚"
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ‹¼å†™çº é”™ä¸“å®¶ï¼Œå¯¹åŸæ–‡è¿›è¡Œé”™åˆ«å­—çº æ­£ï¼Œä¸è¦æ›´æ”¹åŸæ–‡å­—æ•°ï¼ŒåŸæ–‡ä¸ºï¼š"},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# generate outputs
outputs = llm.generate([text], sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}") 
```

## Citation

If this work is helpful, please kindly cite as:

```bibtex

@inproceedings{wei2024ä¸­å°å­¦ä½œæ–‡è¯­æ³•é”™è¯¯æ£€æµ‹,
  title={ä¸­å°å­¦ä½œæ–‡è¯­æ³•é”™è¯¯æ£€æµ‹, ç—…å¥æ”¹å†™ä¸æµç•…æ€§è¯„çº§çš„è‡ªåŠ¨åŒ–æ–¹æ³•ç ”ç©¶},
  author={Wei, Tian},
  booktitle={Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 3: Evaluations)},
  pages={278--284},
  year={2024}
}
```


## Star History

![Star History Chart](https://api.star-history.com/svg?repos=TW-NLP/ChineseErrorCorrector&type=Date)