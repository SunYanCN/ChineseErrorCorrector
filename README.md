#  ä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•çº é”™
[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/README.md) 

<div align="center">
  <a href="https://github.com/TW-NLP/ChineseErrorCorrector">
    <img src="images/image_fx_.jpg" alt="Logo" height="156">
  </a>
</div>



-----------------



## ä»‹ç»
æ”¯æŒä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•é”™è¯¯çº æ­£ï¼Œå¹¶å¼€æºæ‹¼å†™å’Œè¯­æ³•é”™è¯¯çš„å¢å¼ºå·¥å…·ã€å¤§æ¨¡å‹è®­ç»ƒä»£ç ã€‚è£è·2024CCL å† å†› ğŸ†ï¼Œ[æŸ¥çœ‹è®ºæ–‡](https://aclanthology.org/2024.ccl-3.31/) ï¼Œ[2023 NLPCC-NaCGECçº é”™å† å†›ğŸ†](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#nacgec-%E6%95%B0%E6%8D%AE%E9%9B%86)ï¼Œ [2022 FCGEC çº é”™å† å†›ğŸ†](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#fcgec-%E6%95%B0%E6%8D%AE%E9%9B%86) ï¼Œå¦‚æœ‰å¸®åŠ©ï¼Œæ„Ÿè°¢starâœ¨ã€‚

## ğŸ”¥ğŸ”¥ğŸ”¥ æ–°é—»
[2025/04/24] æ ¹æ®[å»ºè®®](https://github.com/TW-NLP/ChineseErrorCorrector/issues/17)ï¼Œæˆ‘ä»¬é‡æ–°è®­ç»ƒçº é”™æ¨¡å‹ï¼Œå¹¶å»æ‰äº†FCGECæ•°æ®é›†ä¸­ä¸nacgecæ•°æ®é›†çš„é‡åˆæ•°æ®ï¼Œå…±è®¡2075æ¡ï¼Œè¶…è¶Šç¬¬ä¸€ååä¸º23ä¸ªç‚¹ï¼Œæ¨¡å‹æƒé‡å·²ç»åŒæ­¥æ›´æ–°ã€‚

[2025/03/28] æ–°å¢å¤§æ¨¡å‹è®­ç»ƒä»£ç ï¼Œä¸åŒé¢†åŸŸå¯ä»¥è®­ç»ƒè‡ªå·±çš„æ–‡æœ¬çº é”™å¤§æ¨¡å‹ï¼Œå¤§å¹…åº¦æé«˜è‡ªå·±çš„é¢†åŸŸçš„çº é”™æ°´å¹³ï¼Œ[è®­ç»ƒæ•™ç¨‹](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#%E8%AE%AD%E7%BB%83) ï¼Œ[æ•°æ®å¢å¼ºå·¥å…·](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/ChineseErrorCorrector/README_DAT.md) ã€‚

[2025/03/17] æ›´æ–°æ‰¹é‡é”™è¯¯æ–‡æœ¬çš„è§£æï¼Œ[transformersæ‰¹é‡è§£æ](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#transformers-%E6%89%B9%E9%87%8F%E6%8E%A8%E7%90%86) ;[VLLMæ‰¹é‡è§£æ](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#vllm-%E5%BC%82%E6%AD%A5%E6%89%B9%E9%87%8F%E6%8E%A8%E7%90%86)

[2025/03/12] ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨ï¼ŒåŸºäºAWQå¯¹[twnlp/ChineseErrorCorrector2-7B](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B)è¿›è¡Œé‡åŒ–ï¼Œå‘å¸ƒ [twnlp/ChineseErrorCorrector2-7B-AWQ](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B-AWQ)ï¼Œåœ¨å•å¼ T4(16G)æ˜¾å¡ä¸Šå³å¯è¿è¡ŒğŸ˜„ï¼Œtransformersæ¨ç†å ç”¨6Gæ˜¾å­˜ï¼Œ [è¿è¡Œå®ä¾‹](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B-AWQ#usage-huggingface-transformers)

[2025/03/10] æ¨¡å‹æ”¯æŒå¤šç§æ¨ç†æ–¹å¼ï¼ŒåŒ…æ‹¬ transformersã€VLLMã€modelscopeã€‚

[2025/02/25] ğŸ‰ğŸ‰ğŸ‰ä½¿ç”¨200ä¸‡çº é”™æ•°æ®è¿›è¡Œå¤šè½®è¿­ä»£è®­ç»ƒï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector2-7B](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B)ï¼Œåœ¨ [NaCGEC-2023NLPCCå®˜æ–¹è¯„æµ‹æ•°æ®é›†](https://github.com/masr2000/NaCGEC)ä¸Šï¼Œè¶…è¶Šç¬¬ä¸€ååä¸º17ä¸ªç‚¹ï¼Œé¥é¥é¢†å…ˆï¼Œæ¨èä½¿ç”¨âœ¨âœ¨ï¼Œ [æŠ€æœ¯è¯¦æƒ…](https://blog.csdn.net/qq_43765734/article/details/145858955)

[2025/02] ä¸ºæ–¹ä¾¿éƒ¨ç½²ï¼Œä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector-1.5B](https://huggingface.co/twnlp/ChineseErrorCorrector-1.5B)

[2025/01] ä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼ŒåŸºäºQwen2.5è®­ç»ƒä¸­æ–‡æ‹¼å†™çº é”™æ¨¡å‹ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰é”™è¯¯çº æ­£ï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector-7B](https://huggingface.co/twnlp/ChineseErrorCorrector-7B)ï¼Œ[twnlp/ChineseErrorCorrector-32B-LORA](https://huggingface.co/twnlp/ChineseErrorCorrector-32B-LORA/tree/main)

[2024/06] v0.1.0ç‰ˆæœ¬ï¼šå¼€æºä¸€é”®è¯­æ³•é”™è¯¯å¢å¼ºå·¥å…·ï¼Œè¯¥å·¥å…·å¯ä»¥è¿›è¡Œ14ç§è¯­æ³•é”™è¯¯çš„å¢å¼ºï¼Œä¸åŒè¡Œä¸šå¯ä»¥æ ¹æ®è‡ªå·±çš„æ•°æ®è¿›è¡Œé”™è¯¯æ›¿æ¢ï¼Œæ¥è®­ç»ƒè‡ªå·±çš„è¯­æ³•å’Œæ‹¼å†™æ¨¡å‹ã€‚è¯¦è§[Tag-v0.1.0](https://github.com/TW-NLP/ChineseErrorCorrector/tree/0.1.0)

## æ¨¡å‹åˆ—è¡¨
| æ¨¡å‹åç§°     |  çº é”™ç±»å‹                                                                                                           | æè¿°          |
|:--------------|:--------------------------------------------------------------------------------------------------------------------------|:------------|
| [twnlp/ChineseErrorCorrector2-7B](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B) |è¯­æ³•+æ‹¼å†™ | ä½¿ç”¨200ä¸‡çº é”™æ•°æ®è¿›è¡Œå¤šè½®è¿­ä»£è®­ç»ƒï¼Œæ•ˆæœå¥½ï¼Œæ¨èä½¿ç”¨ã€‚|
| [twnlp/ChineseErrorCorrector2-7B-AWQ](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B-AWQ) |è¯­æ³•+æ‹¼å†™ | åŸºäºAWQå¯¹ChineseErrorCorrector2-7Bè¿›è¡Œé‡åŒ–ï¼Œé€‚ç”¨äºæ˜¾å­˜ä½çš„æœºå™¨è¿è¡Œã€‚|
| [twnlp/ChineseErrorCorrector-7B](https://huggingface.co/twnlp/ChineseErrorCorrector-7B)|æ‹¼å†™ | ä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰æ‹¼å†™é”™è¯¯çº æ­£ï¼Œæ‹¼å†™çº é”™æ•ˆæœå¥½ã€‚|
| [twnlp/ChineseErrorCorrector-1.5B](https://huggingface.co/twnlp/ChineseErrorCorrector-1.5B)|æ‹¼å†™ | ä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰æ‹¼å†™é”™è¯¯çº æ­£ï¼Œæ‹¼å†™çº é”™æ•ˆæœä¸€èˆ¬ã€‚|



## æ•°æ®é›†

| æ•°æ®é›†åç§°     | æ•°æ®é“¾æ¥                                                                                                      | æ•°æ®é‡å’Œç±»åˆ«è¯´æ˜                                                                                                            | æè¿°          |
|:--------------|:-----------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:------------|
| CSCï¼ˆæ‹¼å†™çº é”™æ•°æ®é›†ï¼‰ |[twnlp/csc_data](https://huggingface.co/datasets/twnlp/csc_data)  | W271Kï¼š279,816 æ¡ï¼ŒMedicalï¼š39,303 æ¡ï¼ŒLemonï¼š22,259 æ¡ï¼ŒECSpellï¼š6,688 æ¡ï¼ŒCSCDï¼š35,001 æ¡ | ä¸­æ–‡æ‹¼å†™çº é”™çš„æ•°æ®é›† |
| CGCï¼ˆè¯­æ³•çº é”™æ•°æ®é›†ï¼‰ |[twnlp/cgc_data](https://huggingface.co/datasets/twnlp/cgc_data)  | CGEDï¼š20449 æ¡ï¼ŒFCGECï¼š37354 æ¡ï¼ŒMuCGECï¼š2467 æ¡ï¼ŒNaSGECï¼š7568æ¡ | ä¸­æ–‡è¯­æ³•çº é”™çš„æ•°æ®é›† |
| Lang8+HSKï¼ˆç™¾ä¸‡è¯­æ–™-æ‹¼å†™å’Œè¯­æ³•é”™è¯¯æ··åˆæ•°æ®é›†ï¼‰ |[twnlp/lang8_hsk](https://huggingface.co/datasets/twnlp/lang8_hsk)  | 1568885æ¡ | ä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•æ•°æ®é›† |



## æ‹¼å†™çº é”™è¯„æµ‹
- è¯„ä¼°æŒ‡æ ‡ï¼šF1


| Model Name       | Model Link                                                                                                              | Base Model                 | Avg        | SIGHAN-2015(é€šç”¨) | EC-LAW(æ³•å¾‹)| EC-MED(åŒ»ç–—)| EC-ODW(å…¬æ–‡)|
|:-----------------|:------------------------------------------------------------------------------------------------------------------------|:---------------------------|:-----------|:------------|:-------|:-------|:--------|
| twnlp/ChineseErrorCorrector-1.5B        | [huggingface](https://huggingface.co/twnlp/ChineseErrorCorrector-1.5B/tree/main)                               | Qwen/Qwen2.5-1.5B-Instruct | 0.459     | 0.346      | 0.517 | 0.433 | 0.540     |
| twnlp/ChineseErrorCorrector-7B        | [huggingface](https://huggingface.co/twnlp/ChineseErrorCorrector-7B/tree/main)                                    | Qwen/Qwen2.5-7B-Instruct | 0.712     | 0.592      | 0.787 | 0.677 | 0.793     |
| twnlp/ChineseErrorCorrector-32B-LORA        | [huggingface](https://huggingface.co/twnlp/ChineseErrorCorrector-32B-LORA/tree/main)                                    | Qwen/Qwen2.5-32B-Instruct |  0.757   |    0.594   | 0.776 |0.794 |   0.864  |

## æ–‡æœ¬çº é”™è¯„æµ‹(åŒå† å†› ğŸ†)
### NaCGEC æ•°æ®é›†
- è¯„ä¼°å·¥å…·ï¼šChERRANT  [è¯„æµ‹å·¥å…·](https://github.com/HillZhang1999/MuCGEC)
- è¯„ä¼°æ•°æ®ï¼š[NaCGEC](https://github.com/masr2000/NaCGEC)
- è¯„ä¼°æŒ‡æ ‡ï¼šF1-0.5

ğŸ†
| Model Name       | Model Link                                                                                                              |    Prec     | Rec | F0.5 |
|:-----------------|:---------------------------------------------------------------|:-----------|:------------|:-------|
|  twnlp/ChineseErrorCorrector2-7B | [huggingface](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B) ï¼› [modelspose(å›½å†…ä¸‹è½½)](https://www.modelscope.cn/models/tiannlp/ChineseErrorCorrector2-7B)       |  0.6867     | 0.6742      | 0.6842 |
|  twnlp/ChineseErrorCorrector2-7B-AWQ | [huggingface](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B-AWQ)       |  0.514     | 0.5671      | 0.5238 |
|  HW_TSC_nlpcc2023_cgec(åä¸º) |   æœªå¼€æº     |  0.5095     | 0.3129      | 0.4526 |
| é±¼é¥¼å•¾å•¾Plus(åŒ—äº¬å¤§å­¦) |   æœªå¼€æº     |  0.5708     | 0.1294      | 0.3394 |
| CUHK_SU(é¦™æ¸¯ä¸­æ–‡å¤§å­¦) |  æœªå¼€æº      |  0.3882     | 0.1558      | 0.2990 |

### FCGEC æ•°æ®é›†
- è¯„ä¼°æŒ‡æ ‡ï¼šbinary_f1

[è¯„æµ‹ğŸ†](https://codalab.lisn.upsaclay.fr/competitions/8020#results)

## ä½¿ç”¨
### ğŸ¤— transformers 

```shell
pip install transformers
```

```shell
from transformers import AutoModelForCausalLM, AutoTokenizer,set_seed
set_seed(42)

model_name = "twnlp/ChineseErrorCorrector2-7B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')

prompt = "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬çº é”™ä¸“å®¶ï¼Œçº æ­£è¾“å…¥å¥å­ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶è¾“å‡ºæ­£ç¡®çš„å¥å­ï¼Œè¾“å…¥å¥å­ä¸ºï¼š"
text_input = "å°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚"
messages = [
    {"role": "user", "content": prompt + text_input}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)

```

### VLLM

```shell
pip install transformers
pip install vllm==0.3.3
```

```shell
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("twnlp/ChineseErrorCorrector2-7B")

# Pass the default decoding hyperparameters of twnlp/ChineseErrorCorrector2-7B
# max_tokens is for the maximum length for generation.
sampling_params = SamplingParams(seed=42,max_tokens=512)

# Input the model name or path. Can be GPTQ or AWQ models.
llm = LLM(model="twnlp/ChineseErrorCorrector2-7B")

# Prepare your prompts
text_input = "å°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚"
messages = [
    {"role": "user", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬çº é”™ä¸“å®¶ï¼Œçº æ­£è¾“å…¥å¥å­ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶è¾“å‡ºæ­£ç¡®çš„å¥å­ï¼Œè¾“å…¥å¥å­ä¸ºï¼š"+text_input}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# generate outputs
outputs = llm.generate([text], sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}") 
```


### VLLM å¼‚æ­¥æ‰¹é‡æ¨ç† 
- Clone the repo
``` sh
git clone https://github.com/TW-NLP/ChineseErrorCorrector
cd ChineseErrorCorrector
```
- Install Conda: please see https://docs.conda.io/en/latest/miniconda.html
- Create Conda env:
``` sh
conda create -n zh_correct -y python=3.9
conda activate zh_correct
pip install -r requirements.txt
# If you are in mainland China, you can set the mirror as follows:
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
```


```sh
# ä¿®æ”¹config.py
#ï¼ˆ1ï¼‰æ ¹æ®ä¸åŒçš„æ¨¡å‹ï¼Œä¿®æ”¹çš„DEFAULT_CKPT_PATHï¼Œé»˜è®¤ä¸ºChineseErrorCorrector2-7B(å°†æ¨¡å‹ä¸‹è½½ï¼Œæ”¾åœ¨ChineseErrorCorrector/pre_model/ChineseErrorCorrector2-7B)
#ï¼ˆ2ï¼‰å°†Qwen2TextCorConfigçš„USE_VLLM = True

#æ‰¹é‡é¢„æµ‹
python main.py
```


### Transformers æ‰¹é‡æ¨ç† 
- Clone the repo
``` sh
git clone https://github.com/TW-NLP/ChineseErrorCorrector
cd ChineseErrorCorrector
```
- Install Conda: please see https://docs.conda.io/en/latest/miniconda.html
- Create Conda env:
``` sh
conda create -n zh_correct -y python=3.9
conda activate zh_correct
pip install -r requirements.txt
# If you are in mainland China, you can set the mirror as follows:
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
```

``` sh
# ä¿®æ”¹config.py
#ï¼ˆ1ï¼‰æ ¹æ®ä¸åŒçš„æ¨¡å‹ï¼Œä¿®æ”¹çš„DEFAULT_CKPT_PATHï¼Œé»˜è®¤ä¸ºChineseErrorCorrector2-7B
#ï¼ˆ2ï¼‰å°†Qwen2TextCorConfigçš„USE_VLLM = False

#æ‰¹é‡é¢„æµ‹
python main.py

#è¾“å‡ºï¼š
'''
[{'source': 'å°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚', 'target': 'å°‘å…ˆé˜Ÿå‘˜åº”è¯¥ä¸ºè€äººè®©åº§ã€‚', 'errors': [('å› ', 'åº”', 4), ('å', 'åº§', 10)]}, {'source': 'å¤§çº¦åŠä¸ªå°æ—¶å·¦å³', 'target': 'å¤§çº¦åŠä¸ªå°æ—¶', 'errors': [('å·¦å³', '', 6)]}]
'''

```


### ğŸ¤– modelscope 

```shell
pip install modelscope
```

```shell
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_name = "tiannlp/ChineseErrorCorrector2-7B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬çº é”™ä¸“å®¶ï¼Œçº æ­£è¾“å…¥å¥å­ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶è¾“å‡ºæ­£ç¡®çš„å¥å­ï¼Œè¾“å…¥å¥å­ä¸ºï¼š"
text_input = "å°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚"
messages = [
    {"role": "user", "content": prompt + text_input}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)

```


## è®­ç»ƒ
### ç¯å¢ƒå‡†å¤‡
- Clone the repo
``` sh
git clone https://github.com/TW-NLP/ChineseErrorCorrector
cd ChineseErrorCorrector
```
- Install Conda: please see https://docs.conda.io/en/latest/miniconda.html
- Create Conda env:
``` sh
conda create -n zh_correct -y python=3.9
conda activate zh_correct
pip install -r requirements.txt
# If you are in mainland China, you can set the mirror as follows:
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
```

### ä¸€é”®è®­ç»ƒ
``` sh
bash ./llm/train/run.sh
``` 

## Citation

If this work is helpful, please kindly cite as:

```bibtex

@inproceedings{wei2024ä¸­å°å­¦ä½œæ–‡è¯­æ³•é”™è¯¯æ£€æµ‹,
  title={ä¸­å°å­¦ä½œæ–‡è¯­æ³•é”™è¯¯æ£€æµ‹, ç—…å¥æ”¹å†™ä¸æµç•…æ€§è¯„çº§çš„è‡ªåŠ¨åŒ–æ–¹æ³•ç ”ç©¶},
  author={Wei, Tian},
  booktitle={Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 3: Evaluations)},
  pages={278--284},
  year={2024}
}
```
## Star History

![Star History Chart](https://api.star-history.com/svg?repos=TW-NLP/ChineseErrorCorrector&type=Date)
