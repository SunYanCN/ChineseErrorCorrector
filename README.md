#  ä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•çº é”™
[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/README.md) 

<div align="center">
  <a href="https://github.com/TW-NLP/ChineseErrorCorrector">
    <img src="images/image_fx_.jpg" alt="Logo" height="156">
  </a>
</div>



-----------------



## ä»‹ç»
æ”¯æŒä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•é”™è¯¯çº æ­£ï¼Œå¹¶å¼€æºæ‹¼å†™å’Œè¯­æ³•é”™è¯¯çš„å¢å¼ºå·¥å…·ï¼Œè£è·2024CCL å† å†› ğŸ†ï¼Œ[æŸ¥çœ‹è®ºæ–‡](https://aclanthology.org/2024.ccl-3.31/) ã€‚

## ğŸ”¥ğŸ”¥ğŸ”¥ æ–°é—»

[2025/02/25] ä½¿ç”¨200ä¸‡çº é”™æ•°æ®è¿›è¡Œå¤šè½®è¿­ä»£è®­ç»ƒï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector2-7B](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B)ï¼Œåœ¨ [NaCGEC-2023NLPCCå®˜æ–¹è¯„æµ‹æ•°æ®é›†](https://github.com/masr2000/NaCGEC)ä¸Šï¼Œè¶…è¶Šç¬¬ä¸€ååä¸º10ä¸ªç‚¹ï¼Œé¥é¥é¢†å…ˆï¼Œæ¨èä½¿ç”¨ï¼

[2025/02] ä¸ºæ–¹ä¾¿éƒ¨ç½²ï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector-1.5B](https://huggingface.co/twnlp/ChineseErrorCorrector-1.5B)

[2025/01] ä½¿ç”¨38ä¸‡å¼€æºæ•°æ®ï¼ŒåŸºäºQwen2.5è®­ç»ƒä¸­æ–‡æ‹¼å†™çº é”™æ¨¡å‹ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰é”™è¯¯çº æ­£ï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector-7B](https://huggingface.co/twnlp/ChineseErrorCorrector-7B)ï¼Œ[twnlp/ChineseErrorCorrector-32B-LORA](https://huggingface.co/twnlp/ChineseErrorCorrector-32B-LORA/tree/main)

[2024/06] v0.1.0ç‰ˆæœ¬ï¼šå¼€æºä¸€é”®è¯­æ³•é”™è¯¯å¢å¼ºå·¥å…·ï¼Œè¯¥å·¥å…·å¯ä»¥è¿›è¡Œ14ç§è¯­æ³•é”™è¯¯çš„å¢å¼ºï¼Œä¸åŒè¡Œä¸šå¯ä»¥æ ¹æ®è‡ªå·±çš„æ•°æ®è¿›è¡Œé”™è¯¯æ›¿æ¢ï¼Œæ¥è®­ç»ƒè‡ªå·±çš„è¯­æ³•å’Œæ‹¼å†™æ¨¡å‹ã€‚è¯¦è§[Tag-v0.1.0](https://github.com/TW-NLP/ChineseErrorCorrector/tree/0.1.0)

## æ•°æ®é›†

| æ•°æ®é›†åç§°     | æ•°æ®é“¾æ¥                                                                                                      | æ•°æ®é‡å’Œç±»åˆ«è¯´æ˜                                                                                                            | æè¿°          |
|:--------------|:-----------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:------------|
| CSCï¼ˆæ‹¼å†™çº é”™æ•°æ®é›†ï¼‰ |[twnlp/csc_data](https://huggingface.co/datasets/twnlp/csc_data)  | W271Kï¼š279,816 æ¡ï¼ŒMedicalï¼š39,303 æ¡ï¼ŒLemonï¼š22,259 æ¡ï¼ŒECSpellï¼š6,688 æ¡ï¼ŒCSCDï¼š35,001 æ¡ | ä¸­æ–‡æ‹¼å†™çº é”™çš„æ•°æ®é›† |
| CGCï¼ˆè¯­æ³•çº é”™æ•°æ®é›†ï¼‰ |[twnlp/cgc_data](https://huggingface.co/datasets/twnlp/cgc_data)  | CGEDï¼š20449 æ¡ï¼ŒFCGECï¼š37354 æ¡ï¼ŒMuCGECï¼š2467 æ¡ï¼ŒNaSGECï¼š7568æ¡ | ä¸­æ–‡è¯­æ³•çº é”™çš„æ•°æ®é›† |
| Lang8+HSKï¼ˆç™¾ä¸‡è¯­æ–™-æ‹¼å†™å’Œè¯­æ³•é”™è¯¯æ··åˆæ•°æ®é›†ï¼‰ |[twnlp/lang8_hsk](https://huggingface.co/datasets/twnlp/lang8_hsk)  | 1568885æ¡ | ä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•æ•°æ®é›† |



## æ‹¼å†™çº é”™è¯„æµ‹
- è¯„ä¼°æŒ‡æ ‡ï¼šF1


| Model Name       | Model Link                                                                                                              | Base Model                 | Avg        | SIGHAN-2015(é€šç”¨) | EC-LAW(æ³•å¾‹)| EC-MED(åŒ»ç–—)| EC-ODW(å…¬æ–‡)|
|:-----------------|:------------------------------------------------------------------------------------------------------------------------|:---------------------------|:-----------|:------------|:-------|:-------|:--------|
| twnlp/ChineseErrorCorrector-1.5B        | https://huggingface.co/twnlp/ChineseErrorCorrector-1.5B/tree/main                                    | Qwen/Qwen2.5-1.5B-Instruct | 0.459     | 0.346      | 0.517 | 0.433 | 0.540     |
| twnlp/ChineseErrorCorrector-7B        | https://huggingface.co/twnlp/ChineseErrorCorrector-7B/tree/main                                    | Qwen/Qwen2.5-7B-Instruct | 0.712     | 0.592      | 0.787 | 0.677 | 0.793     |
| twnlp/ChineseErrorCorrector-32B-LORA        | https://huggingface.co/twnlp/ChineseErrorCorrector-32B-LORA/tree/main                                    | Qwen/Qwen2.5-32B-Instruct |  0.757   |    0.594   | 0.776 |0.794 |   0.864  |

## æ–‡æœ¬çº é”™è¯„æµ‹ï¼ˆæ‹¼å†™é”™è¯¯+è¯­æ³•é”™è¯¯ï¼‰
- è¯„ä¼°å·¥å…·ï¼šChERRANT  [è¯„æµ‹å·¥å…·](https://github.com/HillZhang1999/MuCGEC) 
- è¯„ä¼°æŒ‡æ ‡ï¼šF1-0.5(è¯­æ³•)ã€F1(æ‹¼å†™)


| Test Dataset       | Model Name       | Model Link                                                                                                              | Base Model                 |    Prec     | Rec | F0.5 |
|:-----------------|:-----------------|:------------------------------------------------------------------------------------------------------------------------|:---------------------------|:-----------|:------------|:-------|
| NaSGEC-NLPCC2023    |  twnlp/ChineseErrorCorrector2-7B | https://huggingface.co/twnlp/ChineseErrorCorrector2-7B       | Qwen/Qwen2.5-7B-Instruct                                  |  | å¾…è¯„æµ‹     | å¾…è¯„æµ‹      | å¾…è¯„æµ‹ |


## ä½¿ç”¨
### transformers 
```shell
# pip install transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
checkpoint = "twnlp/ChineseErrorCorrector-7B"

device = "cuda" # for GPU usage or "cpu" for CPU usage
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)

input_content = "ä½ æ˜¯ä¸€ä¸ªæ‹¼å†™çº é”™ä¸“å®¶ï¼Œå¯¹åŸæ–‡è¿›è¡Œé”™åˆ«å­—çº æ­£ï¼Œä¸è¦æ›´æ”¹åŸæ–‡å­—æ•°ï¼ŒåŸæ–‡ä¸ºï¼š\nå°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚"

messages = [{"role": "user", "content": input_content}]
input_text=tokenizer.apply_chat_template(messages, tokenize=False)

print(input_text)

inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)
outputs = model.generate(inputs, max_new_tokens=1024, temperature=0, do_sample=False, repetition_penalty=1.08)

print(tokenizer.decode(outputs[0]))

```

### VLLM

```shell
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("twnlp/ChineseErrorCorrector-7B")

# Pass the default decoding hyperparameters of twnlp/ChineseErrorCorrector-7B
# max_tokens is for the maximum length for generation.
sampling_params = SamplingParams(repetition_penalty=1.05, max_tokens=512)

# Input the model name or path. Can be GPTQ or AWQ models.
llm = LLM(model="twnlp/ChineseErrorCorrector-7B")

# Prepare your prompts
prompt = "å°‘å…ˆé˜Ÿå‘˜å› è¯¥ä¸ºè€äººè®©åã€‚"
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ‹¼å†™çº é”™ä¸“å®¶ï¼Œå¯¹åŸæ–‡è¿›è¡Œé”™åˆ«å­—çº æ­£ï¼Œä¸è¦æ›´æ”¹åŸæ–‡å­—æ•°ï¼ŒåŸæ–‡ä¸ºï¼š"},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

# generate outputs
outputs = llm.generate([text], sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}") 
```

## Citation

If this work is helpful, please kindly cite as:

```bibtex

@inproceedings{wei2024ä¸­å°å­¦ä½œæ–‡è¯­æ³•é”™è¯¯æ£€æµ‹,
  title={ä¸­å°å­¦ä½œæ–‡è¯­æ³•é”™è¯¯æ£€æµ‹, ç—…å¥æ”¹å†™ä¸æµç•…æ€§è¯„çº§çš„è‡ªåŠ¨åŒ–æ–¹æ³•ç ”ç©¶},
  author={Wei, Tian},
  booktitle={Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 3: Evaluations)},
  pages={278--284},
  year={2024}
}
```


## Star History

![Star History Chart](https://api.star-history.com/svg?repos=TW-NLP/ChineseErrorCorrector&type=Date)